{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(account_name):\n",
    "    \n",
    "    # Fill in the brand account name to assemble the url\n",
    "    brand_account_name = account_name\n",
    "    url = f'https://instagram.com/{brand_account_name}/'\n",
    "    \n",
    "    # Visit the profile page\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Record the account info\n",
    "    \n",
    "    ## Get post count, follower count, following count\n",
    "    stat_elements = driver.find_elements_by_class_name('_t98z6')\n",
    "\n",
    "    post_count = int(stat_elements[0].text.split('posts')[0].replace(',', ''))\n",
    "    following_count = int(stat_elements[2].text.split('following')[0].replace(',', ''))\n",
    "\n",
    "    follower_count_broth = stat_elements[1].get_attribute('innerHTML')\n",
    "    follower_soup = BeautifulSoup(follower_count_broth, 'html.parser')\n",
    "    follower_count = int(follower_soup.find('span')['title'].replace(',', ''))\n",
    "    \n",
    "    # Set up list to contain post urls\n",
    "    post_urls_list = []\n",
    "\n",
    "    # Scrape the topmost posts that show up immediately\n",
    "    # and save the urls to the list\n",
    "    # Locate the gallery region of the page\n",
    "    gallery = driver.find_elements_by_css_selector('._havey')[0]\n",
    "    # Save the anchor tags for posts in a list\n",
    "    posts_list = gallery.find_elements_by_css_selector('a')\n",
    "\n",
    "    \n",
    "    for post in posts_list:\n",
    "\n",
    "        post_link = post.get_attribute('href')\n",
    "\n",
    "        if post_link not in post_urls_list:\n",
    "            post_urls_list.append(post_link)\n",
    "    \n",
    "    # Scroll through the rest of the profile to scrape every post\n",
    "    while len(post_urls_list) < post_count:\n",
    "\n",
    "        # Scroll to the bottom to and wait to load more posts\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        # When the browser has trouble loading, \n",
    "        # there may be a pop up to click to retry\n",
    "        try:\n",
    "            driver.find_element_by_link_text('Retry').click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # To compare the url list before and after appending\n",
    "        previous_list_length = len(post_urls_list)\n",
    "\n",
    "        # Scrape and append\n",
    "        # Locate the gallery region of the page\n",
    "        gallery = driver.find_elements_by_css_selector('._havey')[0]\n",
    "        # Save the anchor tags for posts in a list\n",
    "        posts_list = gallery.find_elements_by_css_selector('a')\n",
    "\n",
    "    \n",
    "        for post in posts_list:\n",
    "\n",
    "            post_link = post.get_attribute('href')\n",
    "\n",
    "            if post_link not in post_urls_list:\n",
    "                post_urls_list.append(post_link)\n",
    "\n",
    "        # To compare the url list before and after appending\n",
    "        current_list_length = len(post_urls_list)\n",
    "\n",
    "        # If the list didn't change:\n",
    "        ## Scroll up a little and wait, there may be a timeout\n",
    "        if current_list_length == previous_list_length:\n",
    "            driver.execute_script(\"window.scrollBy(0, -1000);\")\n",
    "            time.sleep(10)\n",
    "\n",
    "        # Check progress\n",
    "        time_check = datetime.strftime(datetime.now(), '%I: %M: %S.%f')\n",
    "        print(f'{account_name} -- {time_check}: {current_list_length} of {post_count}')\n",
    "    \n",
    "    # Return the data if the profile doesn't have a bio link\n",
    "    return {\n",
    "        'account_name': account_name,\n",
    "        'instagram_url': url,\n",
    "        'post_count': post_count,\n",
    "        'follower_count': follower_count,\n",
    "        'following_count': following_count,\n",
    "        'post_urls_list': post_urls_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here are the account names\n",
    "accounts_list = [\n",
    "    'burberry',\n",
    "    'gucci',\n",
    "    'toryburch',\n",
    "    'michaelkors',\n",
    "    'bananarepublic',\n",
    "    'majeofficiel',\n",
    "    'aliceandolivia',\n",
    "    'coach',\n",
    "    'ferragamo',\n",
    "    'chloe'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the accounts and scrape\n",
    "for account in accounts_list:\n",
    "    \n",
    "    data_dict = scrape(account)\n",
    "    \n",
    "    with open(f'{account}.json', 'w') as file:\n",
    "        json.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
